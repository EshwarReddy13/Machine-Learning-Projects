# -*- coding: utf-8 -*-
"""Climate and Covid Cases Clustering using K Means.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RuG4vaYYw3hcgiknODFnC8cfxVxBt5FG

Problem description:
The weather data (March - July 2020) from four different locations (Arizona, Florida, New York, and Washington)  in the United States with varying climatic conditions is utilized to assess effect of weather parameters on the COIVD-19 spread.  
Objective: Cluster the data using K-means algorithm.

# Import the Libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import sklearn

"""# Load the Dataset"""

df = pd.read_csv("COVID-19_Weather.csv")
df.head()

"""# Exploratory Data Analysis"""

print("type before converting: ",df['DATE'].dtype)
df['DATE']=pd.to_datetime(df['DATE'])   #use pd.to_datetime() to convert the column into type datetime
print("type before converting: ",df['DATE'].dtype)

#some values in the column 'CASES_PER_DAY' are negative, we will only consider positive values for our analysis.
df=df[df['CASES_PER_DAY']>=0]

# plot of 'CASES' vs 'DATE' for every state 
data_Arizona = df[df['STATE']=='Arizona'] # create a new DataFrame for all data of Arizona
plt.figure(figsize=(10,10)) #define tbe size of the figure
plt.plot(data_Arizona['DATE'],data_Arizona['CASES']) #plot the timeseries of date vs number of cases.
plt.xlabel("Date")
plt.ylabel("Cases")
plt.title("Arizona")

data_Florida = df[df['STATE']=='Florida'] # create a new DataFrame for all data of Florida
plt.figure(figsize=(10,10))
plt.plot(data_Florida['DATE'],data_Florida['CASES'])
plt.xlabel("Date")
plt.ylabel("Cases")
plt.title("Florida")

data_NY = df[df['STATE']=='New York'] # create a new DataFrame for all data of New York
plt.figure(figsize=(10,10))
plt.plot(data_NY['DATE'],data_NY['CASES'])
plt.xlabel("Date")
plt.ylabel("Cases")
plt.title("New York")

data_Washington = df[df['STATE']=='Washington'] # create a new DataFrame for all data of Washington
plt.figure(figsize=(10,10))
plt.plot(data_Washington['DATE'],data_Washington['CASES'])
plt.xlabel("Date")
plt.ylabel("Cases")
plt.title("Washington")

#plot histogram of average temeprature and windspeed
plt.hist(df['AVG_TEMP'],edgecolor="black")
plt.xlabel("temperature ('F)")
plt.ylabel('frequency')

plt.hist(df['WDSP'],edgecolor="black")
plt.xlabel("Wind Speed (m/s)")
plt.ylabel('frequency')

# scatter plot of total number of daily cases vs average temperature

plt.scatter(df['AVG_TEMP'],df['CASES_PER_DAY'])
plt.xlabel("Average temperature")
plt.ylabel("Cases per day")

# scatter plot of total number of daily cases vs wind speed
plt.scatter(df['WDSP'],df['CASES_PER_DAY'])
plt.xlabel("Windspeed")
plt.ylabel("Cases per day")

"""# Clustering of temperature and number of daily cases data using K-Means clustering"""

# create new data with only cases per day and average temperature
# we wish to cluster the dataset with respect to cases per day and average temperature

data = df[['AVG_TEMP','CASES_PER_DAY']]
data.head()

# get statistical information regarding the dataset
data.describe()

"""Data Standardization"""

# before applying K-Means algorithm on the dataset, we need to standardize the dataset. 

from sklearn.preprocessing import StandardScaler  #import the StandardScaler function
min_max_scaler = StandardScaler() #define an instance of StandardScaler
np_scaled = min_max_scaler.fit_transform(data)  #scale the dataframe
data_scaled = pd.DataFrame(np_scaled)  #convert the scaled input to a DataFrame variable

data_scaled

"""Determining the appropriate K value"""

# calculate the within cluster sum of squares for different number of centroids to generate the sum of squares plot.

from sklearn.cluster import KMeans  #import KMeans library from sklearn

scores=[]   #define an empty list to store the sum of squares for each K value
for i in range(1,20): # select K value from 1 to 20
  k_means = KMeans(n_clusters=i,max_iter=300)  #define the KMeans instance
  '''
  n_clusters = number of clusters (K)
  max_iter = maximum number of iterations for the KMeans algorithm before stopping
  '''

  k_means.fit(data_scaled)   # form the clusters
  scores.append(k_means.inertia_) #k_means.inertia_ returns the within cluster sum of squares 

#plot the scores vs K
plt.plot(scores)
plt.xlim(0, 20)
plt.xticks(np.arange(0, 20, 2))
plt.xlabel('number of clusters (K)')
plt.ylabel('sum of squares')

"""Performing K-Means algorithm on the dataset"""

# choose K=5 (Although the sum of squares is high for K=5, we use this value just for better visibility of clusters)
k_means = KMeans(n_clusters=5,max_iter=1000)  #define the KMeans instance
k_means.fit(data_scaled)  # form the clusters

#create a new column in the dataframe 'data' with elements as the respective clusters
data['cluster']=k_means.predict(data_scaled)
data['cluster'].value_counts()  #get the number of elements in each cluster

data.head()

"""Visualizing K-Means result"""

# create a scatterplot of average temperature vs cases per day 
# The points are colored according to the clusters assigned.
plt.scatter(data['AVG_TEMP'],data['CASES_PER_DAY'],c=data['cluster']) 
plt.xlabel("Temperature")
plt.ylabel("Daily cases")

"""# K-Means using all the features"""

# create new dataset with all the variables except 'DATE' and 'STATE'
data = df[['AVG_TEMP','WDSP','PRCP'	,'DAILY_AQI',	'PM','CASES_PER_DAY']]

np_scaled = min_max_scaler.fit_transform(data)  #scale the dataframe
data_scaled = pd.DataFrame(np_scaled)  #convert the scaled input to a DataFrame variable

scores=[]   #define an empty list to store the sum of squares for each K value
for i in range(1,20): # select K value from 1 to 20
  k_means = KMeans(n_clusters=i,max_iter=300,random_state=48)  #define the KMeans instance
  '''
  n_clusters = number of clusters (K)
  max_iter = maximum number of iterations for the KMeans algorithm before stopping
  '''

  k_means.fit(data_scaled)   # form the clusters
  scores.append(k_means.inertia_) #k_means.inertia_ returns the sum of square of distance of data points from the cluster center

#plot the scores vs K
plt.plot(scores)
plt.xlim(0, 20)
plt.xticks(np.arange(0, 20, 2))
plt.xlabel('number of clusters (K)')
plt.ylabel('sum of squares')

# choose K=3 
k_means = KMeans(n_clusters=3,max_iter=500,random_state=10)  #define the KMeans instance. Initialize a random state for reproducility
k_means.fit(data_scaled)  

#create a new column in the dataframe 'data' with elements as the respective clusters
data['cluster']=k_means.predict(data_scaled)
data['cluster'].value_counts()  #get the number of elements in each cluster

cluster1 = data[data['cluster']==0]
cluster1
# we can see that for moderate temperatures,low wind speed and low AQI, the number of cases is relatively low

print("number of days having daily cases > 300:")
len(cluster1[cluster1['CASES_PER_DAY']>300])

cluster1.describe()

cluster2 = data[data['cluster']==1]
cluster2
#we can see that for moderate temperatures,low wind speed and high AQI, the number of cases is higher as
#compared to cluster 1.

print("number of days having daily cases > 300:")
len(cluster2[cluster2['CASES_PER_DAY']>300])

cluster2.describe()

cluster3 = data[data['cluster']==2]
cluster3
#for low temperatures,high wind speed and high Air Quality Index, the cases are relatively high